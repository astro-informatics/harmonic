{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/astro-informatics/harmonic/blob/main/notebooks/checkpointing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During high-dimensional Bayesian analysis, which is typically computationally heavy, one must frequently run sampling and/or evidence estimation for long periods of time. \n",
    "\n",
    "One issue often encountered is that the computing facilities may go offline or timeout during this period, thus discarding any values computed already. \n",
    "\n",
    "To avoid this issue harmonic supports *checkpointing*, which allows the user to periodically *save* the progress of the computation, and then resume from the saved point (the *checkpoint*). \n",
    "\n",
    "This example illustrates how one may use harmonic with checkpointing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic problem setup\n",
    "\n",
    "As always, import the relevant python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import sys\n",
    "sys.path.insert(0, '../../..')\n",
    "import harmonic as hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Bayesian posterior function (here we'll use a simple Gaussian example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(x, inv_cov):\n",
    "    \"\"\"Compute log_e of n dimensional multivariate gaussian.\n",
    "    \n",
    "    Args: \n",
    "    \n",
    "        x: Position at which to evaluate prior. \n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        double: Value of posterior at x.\n",
    "        \n",
    "    \"\"\"\n",
    "    return -np.dot(x,np.dot(inv_cov,x))/2.0   \n",
    "\n",
    "def init_cov(ndim): \n",
    "    \"\"\"Initialise random diagonal covariance matrix.\n",
    "    \n",
    "    Args: \n",
    "    \n",
    "        ndim: Dimension of Gaussian.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        cov: Covariance matrix of shape (ndim,ndim).\n",
    "        \n",
    "    \"\"\"\n",
    "    cov = np.zeros((ndim,ndim))\n",
    "    diag_cov = np.ones(ndim)\n",
    "    np.fill_diagonal(cov, diag_cov)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the final function init_cov is used to randomly assign a diagonal covariance proportional to the identiy matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define parameters for [emcee](https://emcee.readthedocs.io/en/stable/) and the covariance of our Gaussian example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for emcee sampling\n",
    "ndim = 10                   # number of dimensions\n",
    "nchains = 200               # total number of chains to compute\n",
    "samples_per_chain = 5000    # number of samples per chain\n",
    "nburn = 2000                # number of samples to discard as burn in\n",
    "\n",
    "# initialise random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create covariance matrix \n",
    "cov = init_cov(ndim)\n",
    "inv_cov = np.linalg.inv(cov) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For absolute simplicity let's use the harmonic HyperSphere model and rather than learn the model from training samples we will simply fix the radius of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate hyper-sphere model\n",
    "domains_sphere = [np.array([1E0,1E0])] # Not used since not training model.\n",
    "model = hm.model.HyperSphere(ndim, domains_sphere) \n",
    "\n",
    "# Fit model by hand by fixing radius\n",
    "model.set_R(np.sqrt(ndim)) # A good rule of thumb for a Gaussian with a diagonal covariance\n",
    "model.fitted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Now we need to run the sampler to collect samples but we wish to checkpoint periodically to protect against system crashes or timeouts. One simple way to do this is to execute the following loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial random position and state\n",
    "pos = np.random.rand(ndim * nchains).reshape((nchains, ndim)) * 0.1   \n",
    "rstate = np.random.get_state()\n",
    "\n",
    "# Define how often to checkpoint the evidence class\n",
    "chain_iterations = 10\n",
    "\n",
    "# Instantiate the evidence class\n",
    "cal_ev = hm.Evidence(nchains, model)\n",
    "\n",
    "for chain_i in range(chain_iterations):\n",
    "    # Run the emcee sampler from previous endpoint\n",
    "    sampler = emcee.EnsembleSampler(nchains, ndim, ln_posterior, args=[inv_cov])\n",
    "    (pos, prob, rstate) = sampler.run_mcmc(pos, samples_per_chain/chain_iterations, rstate0=rstate)\n",
    "    \n",
    "    # Collect and format samples\n",
    "    samples = np.ascontiguousarray(sampler.chain[:,:,:])\n",
    "    lnprob = np.ascontiguousarray(sampler.lnprobability[:,:])\n",
    "    chains = hm.Chains(ndim)\n",
    "    chains.add_chains_3d(samples, lnprob)\n",
    "    \n",
    "    # 1) Deserialize the Evidence class\n",
    "    if chain_i > 0:\n",
    "        cal_ev = hm.Evidence.deserialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
    "    \n",
    "    # 2) Add these new chains to Evidence class\n",
    "    cal_ev.add_chains(chains)\n",
    "    \n",
    "    # 3) Serialize the Evidence Class \n",
    "    cal_ev.serialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
    "    \n",
    "    # Clear memory \n",
    "    del chains, samples, lnprob, sampler, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it is not typically necessary to deserialize the Evidence class following each checkpoint but only if execution halts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Bayesian evidence\n",
    "\n",
    "Finally we simply compute the learnt harmonic mean estimator.  Here, we elect to compute the evidence in log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_evidence, ln_evidence_std = cal_ev.compute_ln_evidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple Gaussian example considered, it's also straightforward to compute the evidence analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_analytic_evidence(ndim, cov):\n",
    "    \"\"\"Compute analytic ln_e evidence.\n",
    "    \n",
    "    Args: \n",
    "    \n",
    "        ndim: Dimensionality of the multivariate Gaussian posterior\n",
    "        \n",
    "        cov: Covariance matrix dimension nxn.  \n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        double: Value of posterior at x.\n",
    "        \n",
    "    \"\"\"\n",
    "    ln_norm_lik = -0.5*ndim*np.log(2*np.pi)-0.5*np.log(np.linalg.det(cov))   \n",
    "    return -ln_norm_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the evidence analytically so that we can compare it to the value computed by harmonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_evidence_analytic = ln_analytic_evidence(ndim, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the value computed by harmonic and analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_evidence_err = np.log(np.exp(ln_evidence) + np.exp(ln_evidence_std)) - ln_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_evidence (harmonic) = 9.191328725973255 +/- 0.0032251202241564414\n",
      "ln_evidence (analytic) = 9.189385332046726\n",
      "nsigma = 0.6025803044405069\n"
     ]
    }
   ],
   "source": [
    "print('ln_evidence (harmonic) = {} +/- {}'.format(ln_evidence, ln_evidence_err))\n",
    "print('ln_evidence (analytic) = {}'.format(ln_evidence_analytic))\n",
    "print('nsigma = {}'.format(np.abs(ln_evidence - ln_evidence_analytic) / ln_evidence_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the evidence computed by harmonic is close to that computed analytically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
