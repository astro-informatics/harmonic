{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astro-informatics/harmonic/blob/main/notebooks/checkpointing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCvO91ZUA7o"
      },
      "source": [
        "# Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Dj59rmumE-"
      },
      "source": [
        "During high-dimensional Bayesian analysis, which is typically computationally heavy, one must frequently run sampling and/or evidence estimation for long periods of time. \n",
        "\n",
        "One issue often encountered is that the computing facilities may go offline or timeout during this period, thus discarding any values computed already. \n",
        "\n",
        "To avoid this issue `harmonic` supports *checkpointing*, which allows the user to periodically *save* the progress of the computation, and then resume from the saved point (the *checkpoint*). \n",
        "\n",
        "This interactive tutorial illustrates how one may use `harmonic` with checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSCBJ_S4XvDe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install packages\n",
        "%pip install harmonic emcee corner getdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1OtVkBf7OAIF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Import modules\n",
        "import numpy as np\n",
        "import harmonic as hm\n",
        "import emcee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bTCRZLeXULH"
      },
      "source": [
        "## Basic problem setup\n",
        "\n",
        "Define a Bayesian posterior function (here we'll use a simple Gaussian example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8PgO6f4VQSpD"
      },
      "outputs": [],
      "source": [
        "def ln_posterior(x, inv_cov):\n",
        "    \"\"\"Compute log_e of n dimensional multivariate gaussian.\n",
        "    \n",
        "    Args: \n",
        "    \n",
        "        x: Position at which to evaluate prior. \n",
        "        \n",
        "    Returns:\n",
        "    \n",
        "        double: Value of posterior at x.\n",
        "        \n",
        "    \"\"\"\n",
        "    return -np.dot(x,np.dot(inv_cov,x))/2.0   \n",
        "\n",
        "def init_cov(ndim): \n",
        "    \"\"\"Initialise random diagonal covariance matrix.\n",
        "    \n",
        "    Args: \n",
        "    \n",
        "        ndim: Dimension of Gaussian.\n",
        "        \n",
        "    Returns:\n",
        "    \n",
        "        cov: Covariance matrix of shape (ndim,ndim).\n",
        "        \n",
        "    \"\"\"\n",
        "    cov = np.zeros((ndim,ndim))\n",
        "    diag_cov = np.ones(ndim)\n",
        "    np.fill_diagonal(cov, diag_cov)\n",
        "    return cov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Nqkza_YoLu"
      },
      "source": [
        "Note that the final function `init_cov` is used to randomly assign a diagonal covariance proportional to the identiy matrix.\n",
        "\n",
        "Then define parameters for [`emcee`](https://emcee.readthedocs.io/en/stable/) and the covariance of our Gaussian example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xjsxOkX0QSzL"
      },
      "outputs": [],
      "source": [
        "# Define parameters for emcee sampling\n",
        "ndim = 10                   # number of dimensions\n",
        "nchains = 200               # total number of chains to compute\n",
        "samples_per_chain = 5000    # number of samples per chain\n",
        "nburn = 2000                # number of samples to discard as burn in\n",
        "\n",
        "# initialise random seed\n",
        "np.random.seed(1)\n",
        "\n",
        "# Create covariance matrix \n",
        "cov = init_cov(ndim)\n",
        "inv_cov = np.linalg.inv(cov) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj3-pW0UYrDf"
      },
      "source": [
        "Let's use the `harmonic` rational quadratic spline model with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ju_YJlk2QS7x"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "model = hm.model.RQSplineModel(ndim) \n",
        "# How many epochs model will be trained for\n",
        "epochs_num = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAvoFPV8ZJ0Q"
      },
      "source": [
        "## Checkpointing\n",
        "\n",
        "Now we need to run the sampler to collect samples but we wish to checkpoint periodically to protect against system crashes or timeouts. One simple way to do this is to execute the following loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "us1kBuWlQZTy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training NF: 100%|██████████| 4/4 [01:45<00:00, 26.27s/it]\n"
          ]
        }
      ],
      "source": [
        "# Set initial random position and state\n",
        "pos = np.random.rand(ndim * nchains).reshape((nchains, ndim)) * 0.1   \n",
        "rstate = np.random.get_state()\n",
        "\n",
        "# Define how often to checkpoint the evidence class\n",
        "chain_iterations = 5\n",
        "\n",
        "# Discard burn in\n",
        "sampler = emcee.EnsembleSampler(nchains, ndim, ln_posterior, args=[inv_cov])\n",
        "(pos, prob, rstate) = sampler.run_mcmc(pos, nburn, rstate0=rstate)\n",
        "\n",
        "\n",
        "for chain_i in range(chain_iterations):\n",
        "    # Run the emcee sampler from previous endpoint\n",
        "    sampler = emcee.EnsembleSampler(nchains, ndim, ln_posterior, args=[inv_cov])\n",
        "    (pos, prob, rstate) = sampler.run_mcmc(pos, int(samples_per_chain/chain_iterations), rstate0=rstate)\n",
        "    \n",
        "    # Collect and format samples\n",
        "    samples = sampler.chain\n",
        "    lnprob = sampler.lnprobability\n",
        "    chains = hm.Chains(ndim)\n",
        "    chains.add_chains_3d(samples, lnprob)\n",
        "    \n",
        "    if chain_i == 0:\n",
        "        # Fit model on first set of samples\n",
        "        model.fit(samples.reshape((-1,ndim)), epochs = epochs_num, verbose = True)\n",
        "\n",
        "        # Instantiate the evidence class\n",
        "        cal_ev = hm.Evidence(nchains, model)\n",
        "\n",
        "        # Serialize the Evidence Class \n",
        "        cal_ev.serialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
        "        \n",
        "    # After model is trained, start calculating the evidence\n",
        "    else:\n",
        "        # 1) Deserialize the Evidence class\n",
        "        cal_ev = hm.Evidence.deserialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
        "        # 2) Add these new chains to Evidence class\n",
        "        cal_ev.add_chains(chains)\n",
        "        \n",
        "        # 3) Serialize the Evidence Class \n",
        "        cal_ev.serialize(\".temp.gaussian_example_{}.dat\".format(ndim))\n",
        "    \n",
        "    # Clear memory \n",
        "    del chains, samples, lnprob, sampler, prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HBc2vhRZh8o"
      },
      "source": [
        "Of course, it is not typically necessary to deserialize the `Evidence` class following each checkpoint but only if execution halts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuYYqUYoy5bc"
      },
      "source": [
        "## Compute the Bayesian evidence\n",
        "\n",
        "Finally we simply compute the learnt harmonic mean estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Eel3bSORQZW0"
      },
      "outputs": [],
      "source": [
        "ln_evidence = - cal_ev.ln_evidence_inv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6v_b7YZnEo"
      },
      "source": [
        "## Results\n",
        "\n",
        "For the simple Gaussian example considered, it's also straightforward to compute the evidence analytically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eSxxNW1KQZZc"
      },
      "outputs": [],
      "source": [
        "def ln_analytic_evidence(ndim, cov):\n",
        "    \"\"\"Compute analytic ln_e evidence.\n",
        "    \n",
        "    Args: \n",
        "    \n",
        "        ndim: Dimensionality of the multivariate Gaussian posterior\n",
        "        \n",
        "        cov: Covariance matrix dimension nxn.  \n",
        "        \n",
        "    Returns:\n",
        "    \n",
        "        double: Value of posterior at x.\n",
        "        \n",
        "    \"\"\"\n",
        "    ln_norm_lik = -0.5*ndim*np.log(2*np.pi)-0.5*np.log(np.linalg.det(cov))   \n",
        "    return -ln_norm_lik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsD1ymV2aK3u"
      },
      "source": [
        "Let's also compute the evidence analytically so that we can compare it to the value computed by `harmonic`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "efPNgW8qQZcW"
      },
      "outputs": [],
      "source": [
        "ln_evidence_analytic = ln_analytic_evidence(ndim, cov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0UhPgtWaUpb"
      },
      "source": [
        "Let's compare the value computed by `harmonic` and analytically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WrB47hA3QZfM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ln_evidence_err_neg = -0.0031277022888513755\n",
            "ln_evidence_err_pos = 0.0031179502607592245\n",
            "ln_evidence_err_avg = 0.0031228262748053\n"
          ]
        }
      ],
      "source": [
        "ln_evidence_err_neg, ln_evidence_err_pos = cal_ev.compute_ln_inv_evidence_errors()\n",
        "ln_evidence_err_avg = (ln_evidence_err_pos - ln_evidence_err_neg) / 2.0\n",
        "print(f\"ln_evidence_err_neg = {ln_evidence_err_neg}\")\n",
        "print(f\"ln_evidence_err_pos = {ln_evidence_err_pos}\")\n",
        "print(f\"ln_evidence_err_avg = {ln_evidence_err_avg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP3AjUrCQZhh",
        "outputId": "cc01f114-07e0-4abe-ea2f-72fede5943f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ln_evidence (harmonic) = 9.186614990234375 (+ 0.0031179502607592245 / -0.0031277022888513755)\n",
            "ln_evidence (analytic) = 9.189385332046726\n",
            "nsigma = 0.887126458074755\n"
          ]
        }
      ],
      "source": [
        "print('ln_evidence (harmonic) = {} (+ {} / {})'.format(ln_evidence, ln_evidence_err_pos, ln_evidence_err_neg))\n",
        "print('ln_evidence (analytic) = {}'.format(ln_evidence_analytic))\n",
        "print('nsigma = {}'.format(np.abs(ln_evidence - ln_evidence_analytic) / ln_evidence_err_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_37pQ-HWa1Cf"
      },
      "source": [
        "As expected, the evidence computed by `harmonic` is close to that computed analytically."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPeQz+AuMy/ROmUgYTB/C7J",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "checkpointing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
