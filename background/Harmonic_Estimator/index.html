<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Harmonic Mean Estimator &mdash; Harmonic 1.0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom_tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/dark_mode_css/general.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/dark_mode_css/dark.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/tabs.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script src="../../_static/dark_mode_js/default_light.js"></script>
        <script src="../../_static/dark_mode_js/theme_switcher.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Learnt Container Function" href="../Machine_Learning/index.html" />
    <link rel="prev" title="Installation" href="../../user_guide/install.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html">
            <img src="../../_static/harm_badge_simple.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematics</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Harmonic Mean Estimator</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#re-targeted-harmonic-mean">Re-targeted harmonic mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="#learnt-harmonic-mean">Learnt harmonic mean</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Machine_Learning/index.html">Learnt Container Function</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/index.html">Jupyter Notebooks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples/index.html">Benchmark Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">Namespaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Changelog</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/changes.html">GitHub History</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Harmonic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Harmonic Mean Estimator</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/background/Harmonic_Estimator/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="harmonic-mean-estimator">
<h1>Harmonic Mean Estimator<a class="headerlink" href="#harmonic-mean-estimator" title="Permalink to this headline"></a></h1>
<p>The harmonic mean estimator was first proposed by <a class="reference external" href="https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1994.tb01956.x">Newton &amp; Raftery (1994)</a> who showed that the marginal likelihood <span class="math notranslate nohighlight">\(z\)</span> can be estimated from the harmonic mean of the likelihood, given posterior samples. This follows by considering the expectation of the reciprocal of the likelihood with respect to the posterior distribution:</p>
<div class="math notranslate nohighlight">
\[\rho = \mathbb{E}_{\text{P}(\theta | y)} \biggl[\frac{1}{\mathcal{L}(\theta)} \biggr] = \int \,\text{d} \theta\frac{1}{\mathcal{L}(\theta)} \text{P}(\theta | y) = \int \,\text{d} \theta \frac{1}{\mathcal{L}(\theta)} \frac{\mathcal{L}(\theta) \pi(\theta)}{z} = \frac{1}{z},\]</div>
<p>where the final line follows since the prior <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> is a normalised probability distribution. This relationship between the marginal likelihood and the harmonic mean motivates the <strong>orignal harmonic mean estimator</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat{\rho} = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{\mathcal{L}(\theta_i)}, \quad \theta_i \sim \text{P}(\theta | y),\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> specifies the number of samples <span class="math notranslate nohighlight">\(\theta_i\)</span> drawn from the posterior, and from which the evidence may naively be estimated by <span class="math notranslate nohighlight">\(\hat{z} = 1 / \hat{\rho}\)</span> (for now we simply consider the estimation of the reciprocal of the evidence <span class="math notranslate nohighlight">\(\hat{\rho}\)</span> and discuss estimation of the marginal likelihood and Bayes factors later.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As immediately realised by Neal (1994), this estimator can fail catastrophically since its variance can become very large and may not be finite. Review articles that consider a number of alternative methods to estimate the marginal likelihood have also found that the harmonic mean estimator is not accurate, <em>e.g.</em> <a class="reference external" href="http://adsabs.harvard.edu/pdf/2007ASPC..371..224C">Clyde (2007)</a> and <a class="reference external" href="https://arxiv.org/pdf/1111.1957.pdf">Friel (2012)</a>. To understand why the estimator can lead to extremely large variance we consider an importance sampling interpretation of the harmonic mean estimator.</p>
</div>
<p><strong>Importance sampling interpretation:</strong></p>
<p>The harmonic mean estimator can be interpreted as importance sampling.  Consider the reciprocal marginal likelihood, which may be expressed in terms of the prior and posterior by</p>
<div class="math notranslate nohighlight">
\[\rho = \int \,\text{d} \theta \: \frac{1}{\mathcal{L}(\theta)} \: \text{P}(\theta | y) = \int \,\text{d} \theta \: \frac{1}{z} \: \frac{\pi(\theta)}{\text{P}(\theta | y)} \: \text{P}(\theta | y).\]</div>
<p>It is clear the estimator has an importance sampling interpretation where the importance sampling target distribution is the prior <span class="math notranslate nohighlight">\(\pi(\theta)\)</span>, while the sampling density is the posterior <span class="math notranslate nohighlight">\(\text{P}(\theta | y)\)</span>, in contrast to typical importance sampling scenarios.</p>
<p>For importance sampling to be effective, one requires the sampling density to have fatter tails than the target distribution, <em>i.e.</em> to have greater probability mass in the tails of the distribution. Typically the prior has fatter tails than the posterior since the posterior updates our initial understanding of the underlying parameters <span class="math notranslate nohighlight">\(\theta\)</span> that are encoded in the prior, in the presence of new data <span class="math notranslate nohighlight">\(y\)</span>. For the harmonic mean estimator the importance sampling density (the posterior) typically does <strong>not</strong> have fatter tails than the target (the prior) and so importance sampling is not effective. This explains why the original harmonic mean estimator is problematic. A number of variants of the original harmonic mean estimator have been intorduced in an attempt to address this issue.</p>
<div class="section" id="re-targeted-harmonic-mean">
<h2>Re-targeted harmonic mean<a class="headerlink" href="#re-targeted-harmonic-mean" title="Permalink to this headline"></a></h2>
<p>The original harmonic mean estimator was revised by <a class="reference external" href="https://www.jstor.org/stable/pdf/2346123.pdf?casa_token=AB4ArghUKVEAAAAA:rEgBfQoBtpwJUFYmm07FvgnQoc9V5c07jEkctApAqlzZ1z9M16GCtDlGQsQfL5AzNgaz1YMLlN6-J7VQIy1xET9BtJyaQl_L2PEOXGjOd2MYiP7127g">Gelfand (1994)</a> by introducing an arbitrary density <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> to relate the reciprocal of the marginal likelihood to the likelihood through the following expectation:</p>
<div class="math notranslate nohighlight">
\[\rho = \mathbb{E}_{\text{P}(\theta | y)} \biggl[\frac{\varphi(\theta)}{\mathcal{L}(\theta) \pi(\theta)} \biggr] = \int \,\text{d} \theta\frac{\varphi(\theta)}{\mathcal{L}(\theta) \pi(\theta)} \text{P}(\theta | y) = \int \,\text{d} \theta \frac{\varphi(\theta)}{\mathcal{L}(\theta) \pi(\theta)} \frac{\mathcal{L}(\theta) \pi(\theta)}{z} = \frac{1}{z},\]</div>
<p>where the final line follows since the density <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> must be normalised. The above expression motivates the estimator:</p>
<div class="math notranslate nohighlight">
\[\label{eqn:harmonic_mean_retargeted}
\hat{\rho} =
\frac{1}{N} \sum_{i=1}^N
\frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)},
\quad
\theta_i \sim \text{P}(\theta | y).\]</div>
<p>The normalised density <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> can be interpreted as an alternative importance sampling target distribution, as we will see, hence we refer to this approach as the <em>re-targeted harmonic mean estimator</em>. Note that the original harmonic mean estimator is recovered for the target distribution <span class="math notranslate nohighlight">\(\varphi(\theta) = \pi(\theta)\)</span>.</p>
<p><strong>Importance sampling interpretation:</strong></p>
<p>With the introduction of the distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span>, the importance sampling interpretation of the harmonic mean estimator reads</p>
<div class="math notranslate nohighlight">
\[\rho = \int \,\text{d} \theta \: \frac{\varphi(\theta)}{\mathcal{L}(\theta) \pi(\theta)} \: \text{P}(\theta | y) = \int \,\text{d} \theta \: \frac{1}{z} \: \frac{\varphi(\theta)}{\text{P}(\theta | y)} \: \text{P}(\theta | y).\]</div>
<p>It is clear that the distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> now plays the role of the importance sampling target distribution. One is free to choose <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span>, with the only constraint being that it is a normalised distribution.  It is therefore possible to select the target distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> such that it has narrower tails than the posterior, which we recall plays the role of the importance sampling density, thereby avoiding the problematic configuration of the original harmonic mean estimator.  However, the question of how to develop an effective strategy to select <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> for a given problem remains, which is particularly difficult in high-dimensional settings.</p>
</div>
<div class="section" id="learnt-harmonic-mean">
<h2>Learnt harmonic mean<a class="headerlink" href="#learnt-harmonic-mean" title="Permalink to this headline"></a></h2>
<p>It is well-known that the original harmonic mean estimator can fail catastrophically since the variance of the estimator may be become very large. However, this issue can be resolved by introducing an alternative (normalised) target distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span>, yielding what we term here the <em>re-targeted harmonic mean estimator</em>. From the importance sampling interpretation of the harmonic mean estimator, the re-targeted estimator follows by replacing the importance sampling target of the prior <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> with the target <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span>, where the posterior <span class="math notranslate nohighlight">\(P(\theta | y)\)</span> plays the role of the importance sampling density.</p>
<p>It remains to select a suitable target distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span>. On one hand, to ensure the variance of the resulting estimator is well-behaved, the target distribution should have narrower tails that the importance sampling density, <em>i.e.</em> the target <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> should have narrower tails than the posterior <span class="math notranslate nohighlight">\(P(\theta | y)\)</span>. On the other hand, to ensure the resulting estimator is efficient and makes use of as many samples from the posterior as possible, the target distribution should not be too narrow. The optimal target distribution is the normalised posterior distribution since in this case the variance of the resulting estimator is zero. However, the normalised posterior is not accessible since it requires knowledge of the marginal likelihood, which is precisely the term we are attempting to compute.</p>
<p class="centered">
<strong>We propose learning the target distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> from samples of the posterior. Samples from the posterior can be split into training and evaluation (<em>cf.</em> test) sets. Machine learning (ML) techniques can then be applied to learn an approximate model of the normalised posterior from the training samples, with the constraint that the tails of the learnt target are narrower than the posterior, <em>i.e.</em></strong></p><div class="math notranslate nohighlight">
\[\varphi(\theta) \stackrel{\text{ML}}{\simeq} \varphi^\text{optimal}(\theta) = \frac{\mathcal{L}(\theta) \pi(\theta)}{z}.\]</div>
<p class="centered">
<strong>We term this approach the <em>learnt harmonic mean estimator</em>.</strong></p><p>We are interested not only in an estimator for the marginal likelihood but also in an estimate of the variance of this estimator, and its variance. Such additional estimators are useful in their own right and can also provide valuable sanity checks that the resulting marginal likelihood estimator is well-behaved. We present corresponding estimators for the cases of uncorrelated and correlated samples.
Harmonic mean estimators provide an estimation of the reciprocal of the marginal likelihood.  We therefore also consider estimation of the marginal likelihood itself and its variance from the reciprocal estimators.  Moreover, we present expressions to also estimate the Bayes factor, and its variance, to compare two models.
Finally, we present models to learn the normalised target distribution <span class="math notranslate nohighlight">\(\varphi(\theta)\)</span> by approximating the posterior distribution, with the constraint that the target has narrower tails than the posterior, and discuss how to train such models.  Training involves constructing objective functions that penalise models that would result in estimators with a large variance, with appropriate regularisation.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Uncorrelated Samples</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Correlated Samples</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>MCMC algorithms that are typically used to sample the posterior distribution result in correlated samples. By suitably thinning the MCMC chain (discarding all but every <span class="math notranslate nohighlight">\(t^{\text{th}}\)</span> sample), however, samples that are uncorrelated can be obtained. In this subsection we present estimators for the reciprocal marginal likelihod and its variance under the assumption of uncorrelated samples from the posterior.</p>
<p>Consider the harmonic moments</p>
<div class="math notranslate nohighlight">
\[\mu_n = \mathbb{E}_{\text{P}(\theta | y)} \Biggl[\biggl(\frac{\varphi(\theta)}{\mathcal{L}(\theta) \pi(\theta)}\biggr)^n \Biggr],\]</div>
<p>and corresponding central moments</p>
<div class="math notranslate nohighlight">
\[\mu_n^\prime = \mathbb{E}_{\text{P}(\theta | y)} \Biggl[\biggl(\frac{\varphi(\theta)}{\mathcal{L}(\theta) \pi(\theta)} - \mathbb{E}_{\text{P}(\theta | y)} \biggl(\frac{\varphi(\theta)}{\mathcal{L}(\theta)\pi(\theta)} \biggr)\biggr)^n\Biggr].\]</div>
<p>We make use of the following harmonic moment estimators computed from samples of the posterior:</p>
<div class="math notranslate nohighlight">
\[\hat{\mu}_n = \frac{1}{N} \sum_{i=1}^N \biggl(\frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)}\biggr)^n,\quad \theta_i \sim \text{P}(\theta | y),\]</div>
<p>which are unbiased estimators of <span class="math notranslate nohighlight">\(\mu_n\)</span>, i.e. <span class="math notranslate nohighlight">\(\mathbb{E}(\hat{\mu}_n) = \mu_n\)</span>.</p>
<p>The reciprocal marginal likelihood can then be estimated from samples of the posterior by</p>
<div class="math notranslate nohighlight">
\[\hat{\rho} = \hat{\mu}_1 = \frac{1}{N} \sum_{i=1}^N \frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)}, \quad \theta_i \sim \text{P}(\theta | y).\]</div>
<p>The mean and variance of the estimator read, respecitvely,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\hat{\rho}) = \mathbb{E} \Biggl [\frac{1}{N} \sum_{i=1}^N
\frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)} \Biggr ] = \mu_1 = \rho\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\text{var}(\hat{\rho}) = \text{var} \Biggl [ \frac{1}{N} \sum_{i=1}^N \frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)} \Biggr ] = \frac{1}{N} (\mu_2 - \mu_1 ^2 ).\]</div>
<p>Note that the estimator is unbiased. The optimal target is given by the normalised posterior, <em>i.e.</em> <span class="math notranslate nohighlight">\(\varphi^\text{optimal}(\theta) = \mathcal{L}(\theta) \pi(\theta)/z\)</span>. It is straightforward to see that in this case</p>
<div class="math notranslate nohighlight">
\[\mu_n = \hat{\mu}_n = \frac{1}{z^n},\]</div>
<p>and thus the target distribution is optimal since</p>
<div class="math notranslate nohighlight">
\[\text{var}(\hat{\rho}) = \frac{1}{N} (\mu_2 - \mu_1 ^2 ) = \frac{1}{N} (1/z^2 - (1/z) ^2 ) = 0.\]</div>
<p>We are interested in not only an estimate of the reciprocal marginal likelihood but also its variance <span class="math notranslate nohighlight">\(\text{var}(\hat{\rho})\)</span>. It is clear that a suitable estimator of the variance is given by</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{1}{N-1} (\hat{\mu}_2 - \hat{\mu}_1 ^2 ) = \frac{1}{N(N-1)} \sum_{i=1}^N \biggl(\frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)}\biggr)^2 - \frac{\hat{\rho}^2}{N-1}.\]</div>
<p>It follows that this estimator of the variance is unbiased since</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\hat{\sigma}^2) = \frac{1}{N} (\mu_2 - \mu_1^2) = \text{var}(\hat{\rho}).\]</div>
<p>The variance of the estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> reads</p>
<div class="math notranslate nohighlight">
\[\text{var}(\hat{\sigma}^2) = \frac{1}{(N-1)^2} \biggl[ \frac{(N - 1)^2}{N^3} \mu_4^\prime - \frac{(N-1)(N-3)}{N^3} \mu_2^\prime{}^2 \biggr],\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_n^\prime\)</span> are central moments, which follows by a well-known result for the variance of a sample variance, see <a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-642-57489-4_66">Rose (2002)</a> page 264. An unbiased estimator of <span class="math notranslate nohighlight">\(\text{var}(\hat{\sigma}^2)\)</span> can be constructioned from h-statistics, which provide unbiased estimators of central moments.</p>
<p>While we have presented general estimators for uncorrelated samples here, generating uncorrelated samples requires thinning the MCMC chain, which is highly inefficient. It is generally recognised that thinning should be avoided when possible since it reduces the precision with which summaries of the MCMC chain can be computed. One may also consider estimators that do not require uncorrelated samples and so can make use of considerably more MCMC samples of the posterior.</p>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>We present an estimator of the reciprocal marginal likelihood, an estimate of the variance of this estimator, and its variance. These estimators make use of correlated samples in order to avoid the loss of efficiency that results from thinning an MCMC chain.</p>
<p>We propose running a number of independent MCMC chains and using all of the correlated samples within a given chain. A number of modern MCMC sampling techniques, such as affine invariance ensemble samplers naturally provide samples from multiple chains by their ensemble nature. Moreoever, excellent software implementations are readily available, such as the <a class="reference external" href="https://emcee.readthedocs.io/en/stable/">emcee</a> code, which provides an implementation of the affine invariance ensember samplers proposed by <a class="reference external" href="https://msp.org/camcos/2010/5-1/camcos-v5-n1-p04-s.pdf">Goodman (2010)</a>. Alternatively, if only a single large chain is available then this can be broken into separate blocks, which are (approximately) indpendent for a suitably long block length. Subsequently, we use the terminology chains throughout to refer to both scenarios of running multiple MCMC chains or separating a single chain in blocks.</p>
<p>Consider <span class="math notranslate nohighlight">\(C\)</span> chains of samples, indexed by <span class="math notranslate nohighlight">\(j = 1, 2, \ldots, C\)</span>, with chain <span class="math notranslate nohighlight">\(j\)</span> containing <span class="math notranslate nohighlight">\(N_j\)</span> samples.  The <span class="math notranslate nohighlight">\(i^{\text{th}}\)</span> sample of chain <span class="math notranslate nohighlight">\(j\)</span> is denoted <span class="math notranslate nohighlight">\(\theta_{ij}\)</span>.  Since the chain of interest is typically clear from the context, for notational brevity we drop the chain index from the samples, <em>i.e.</em> we denotate samples by <span class="math notranslate nohighlight">\(\theta_i\)</span> where the chain of interest is inferred from the context.</p>
<p>An estimator of the reciprocal marginal likelihood can be computed from each independent chain by</p>
<div class="math notranslate nohighlight">
\[\hat{\rho}_j = \frac{1}{N_j} \sum_{i=1}^{N_j} \frac{\varphi(\theta_i)}{\mathcal{L}(\theta_i) \pi(\theta_i)}, \quad \theta_i \sim \text{P}(\theta | y).\]</div>
<p>A single estimator of the reciprocal marginal likelihood can then be constructed from the estimator for each chain by</p>
<div class="math notranslate nohighlight">
\[\hat{\rho} = \frac{\sum_{j=1}^{C} w_j \hat{\rho}_j} {\sum_{j=1}^{C} w_j },\]</div>
<p>where the estimator <span class="math notranslate nohighlight">\(\hat{\rho}_j\)</span> of chain <span class="math notranslate nohighlight">\(j\)</span> is weighted by the number of samples in the chain, <em>i.e.</em> <span class="math notranslate nohighlight">\(w_j = N_j\)</span>.  It is straightforward to see that the estimator of the reciprocal marginal likelihood is unbiased, <em>i.e.</em> <span class="math notranslate nohighlight">\(\mathbb{E}(\hat{\rho})= \rho\)</span>, since <span class="math notranslate nohighlight">\(\mathbb{E}(\hat{\rho}_j) = \rho\)</span>.</p>
<p>The variance of the estimator <span class="math notranslate nohighlight">\(\hat{\rho}\)</span> is related to the population variance <span class="math notranslate nohighlight">\(\sigma^2 = \mathbb{E}\bigl[ (\hat{\rho}_i - \mathbb{E}(\hat{\rho}_i))^2 \bigr]\)</span> by</p>
<div class="math notranslate nohighlight">
\[\text{var}(\hat{\rho}) = \frac{\sigma^2}{N_\text{eff}},\]</div>
<p>where the effective sample size is given by</p>
<div class="math notranslate nohighlight">
\[N_\text{eff} = \frac{\bigl(\sum_j^{C} w_j \bigr)^2}{\sum_j^{C} w_j^2}.\]</div>
<p>The estimator of the population variance, given by</p>
<div class="math notranslate nohighlight">
\[\hat{s}^2 = \frac{N_\text{eff}} {N_\text{eff}-1} \frac{\sum_{j=1}^{C} w_j (\hat{\rho}_j-\hat{\rho})^2}{\sum_j^{C} w_j},\]</div>
<p>is unbiased, <em>i.e.</em> <span class="math notranslate nohighlight">\(\mathbb{E}(\hat{s}^2) = \sigma^2\)</span>. A suitable estimator for <span class="math notranslate nohighlight">\(\text{var}(\hat{\rho})\)</span> is thus</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{\hat{s}^2}{N_\text{eff}} = \frac{1} {N_\text{eff}-1} \frac{\sum_{j=1}^{C} w_j (\hat{\rho}_j-\hat{\rho})^2}{\sum_j^{C} w_j},\]</div>
<p>which is unbiased, <em>i.e.</em> <span class="math notranslate nohighlight">\(\mathbb{E}(\hat{\sigma}^2) = \text{var}(\hat{\rho})\)</span>, since <span class="math notranslate nohighlight">\(\hat{s}^2\)</span> is unbiased. The variance of the estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> reads</p>
<div class="math notranslate nohighlight">
\[\text{var}(\hat{\sigma}^2) = \frac{1}{N_\text{eff}{}^2} \text{var}(\hat{s}^2) = \frac{\sigma^4}{N_\text{eff}{}^3} \biggl(\kappa - 1 + \frac{2}{N_\text{eff}-1}\biggr),\]</div>
<p>where in the second equality we have used a well-known result for the variance of the sample variance of independent and identically distributed (i.i.d.) random variables.
The kurtosis <span class="math notranslate nohighlight">\(\kappa\)</span> is defined by</p>
<div class="math notranslate nohighlight">
\[\kappa = \text{kur}(\hat{\rho}_i) = \mathbb{E} \Biggl[ \biggl(\frac{\hat{\rho}_i - \rho}{\sigma}\biggr)^4 \Biggr].\]</div>
<p>A suitable estimator for <span class="math notranslate nohighlight">\(\text{var}(\hat{\sigma}^2)\)</span> is thus</p>
<div class="math notranslate nohighlight">
\[\hat{\nu}^4 = \frac{\hat{s}^4}{N_\text{eff}{}^3} \biggl(\hat{\kappa} - 1 + \frac{2}{N_\text{eff}-1}\biggr) = \frac{\hat{\sigma}^4}{N_\text{eff}{}} \biggl(\hat{\kappa} - 1 + \frac{2}{N_\text{eff}-1}\biggr),\]</div>
<p>where for the kurtosis we adopt the estimator</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\kappa} = \frac{\sum_{j=1}^{C} w_j (\hat{\rho}_j-\hat{\rho})^4} {\hat{s}^4 \sum_{j=1}^{C} w_j} \\ = \frac{\sum_{j=1}^{C} w_j (\hat{\rho}_j-\hat{\rho})^4} {N_\text{eff}{}^2 \hat{\sigma}^4 \sum_{j=1}^{C} w_j}.\end{split}\]</div>
<p>The estimators <span class="math notranslate nohighlight">\(\hat{\rho}\)</span>, <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> and <span class="math notranslate nohighlight">\(\hat{\nu}^4\)</span> provide a strategy to esimate the reciprocal marginal likelihood, its variance, and the variance of the variance, respectively. The variance estimators provide valuable measures of the accuracy of the estimated reciprocal marginal likelihood and provide useful sanity checks. Additional sanity checks can also be considered.</p>
<p>By the central limit theorem, for a large number of samples the distribution of <span class="math notranslate nohighlight">\(\hat{\rho}_j\)</span> approaches a Gaussian, with kurtosis <span class="math notranslate nohighlight">\(\kappa=3\)</span>.  If the estimated kurtosis <span class="math notranslate nohighlight">\(\hat{\kappa} \gg 3\)</span> it would indicate that the sampled distribution of <span class="math notranslate nohighlight">\(\hat{\rho}_j\)</span> has long tails, suggesting further samples need to be drawn.</p>
<p>Similarly, the ratio of <span class="math notranslate nohighlight">\(\hat{\nu}^2 / \hat{\sigma}^2\)</span> can be inspected to see if it is close to that expected for a Gaussian distribution with <span class="math notranslate nohighlight">\(\kappa=3\)</span> of</p>
<div class="math notranslate nohighlight">
\[\frac{\hat{\nu}^4}{\hat{\sigma}^4} = \frac{1}{N_\text{eff}{}} \biggl(2 + \frac{2}{N_\text{eff}-1}\biggr) = \frac{2}{N_\text{eff}-1}\]</div>
<p>or equivalently</p>
<div class="math notranslate nohighlight">
\[\frac{\hat{\nu}^2}{\hat{\sigma}^2} = \sqrt{\frac{2}{N_\text{eff}-1}}.\]</div>
<p>For the common setting where the number of samples per chain is constant, <em>i.e.</em> <span class="math notranslate nohighlight">\(N_j = N\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>,</p>
<div class="math notranslate nohighlight">
\[N_\text{eff} = \frac{\bigl(\sum_j^{C} w_j \bigr)^2}{\sum_j^{C} w_j^2} = \frac{(N C)^2}{N^2 C} = C\]</div>
<p>and, say <span class="math notranslate nohighlight">\(C=100\)</span>, we find <span class="math notranslate nohighlight">\(\hat{\nu}^2 / \hat{\sigma}^2 = 0.14\)</span>. In this setting significantly larger values of this ratio would suggest that further samples need to be drawn.</p>
</div></div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../user_guide/install.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../Machine_Learning/index.html" class="btn btn-neutral float-right" title="Learnt Container Function" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Jason D. McEwen, Christopher G. R. Wallis, Matthew A. Price.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>