<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmark Examples &mdash; Harmonic 1.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom_tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/dark_mode_css/general.css" type="text/css" />
      <link rel="stylesheet" href="../_static/dark_mode_css/dark.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/tabs.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script src="../_static/dark_mode_js/default_light.js"></script>
        <script src="../_static/dark_mode_js/theme_switcher.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Namespaces" href="../api/index.html" />
    <link rel="prev" title="Model Cross-validation" href="../tutorials/cross-validation_learnt_model.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/harm_badge_simple.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../background/Harmonic_Estimator/index.html">Harmonic Mean Estimator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../background/Machine_Learning/index.html">Learnt Container Function</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Jupyter Notebooks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Benchmark Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">Namespaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Changelog</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/changes.html">GitHub History</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Harmonic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Benchmark Examples</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="benchmark-examples">
<h1>Benchmark Examples<a class="headerlink" href="#benchmark-examples" title="Permalink to this headline"></a></h1>
<p>To demonstrate the efficacy of <strong>Harmonic</strong> we have included a variety of common evidence estimation examples, where the posterior function is particularly pathological. These examples are somewhat standard benchmarks, and in many cases have historically highlight the failings of the vanilla harmonic mean estimator. Complexity increases from left to right, culminating in the logistic regression and non-nested linear regressions of the Pima Indian and Radiata Pine benchmarks respectively. See <a class="reference external" href="https://arxiv.org/pdf/1111.1957.pdf">Friel and Wyse (2011)</a> for an extensive review of various estimators applied to these benchmarks.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Gaussian</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Rosenbrock</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Rastrigin</button><button aria-controls="panel-0-0-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-3" name="0-3" role="tab" tabindex="-1">Normal Gamma</button><button aria-controls="panel-0-0-4" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-4" name="0-4" role="tab" tabindex="-1">Radiata Pine</button><button aria-controls="panel-0-0-5" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-5" name="0-5" role="tab" tabindex="-1">Pima Indian</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>A standard 2D Gaussian is a simple example often used for basic benchmarking and code validation. The 2D Gaussian posterior configuration here is composed of a likelihood defined as</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x,y) = \frac{1}{2\pi\sigma^2}e^{-\frac{x^2 + y^2}{2\sigma^2}}\]</div>
<p>and a prior defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi(x,y) = \left\{ \begin{array}{lll}     \frac{1}{\pi w^2},  &amp;{\rm if}  \  x^2+y^2 &lt; w^2 \\
0, \quad\quad&amp; {\rm else} \end{array}\right.\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> is a radial weighting function defined such that <span class="math notranslate nohighlight">\(w=1\)</span> inside a circle of radius <span class="math notranslate nohighlight">\(R\)</span> and 0 outside. A useful property of this particular posterior is that it permits a closed for analytic expression for the evidence, given as</p>
<div class="math notranslate nohighlight">
  \begin{align}
  z &amp;= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}dxdy\mathcal{L}(x,y)\pi(x,y),\\
                &amp;= \int_0^{2\pi}\int_0^w rdrd\theta\frac{1}{2\pi\sigma^2} e^{-\frac{r^2}{2\sigma^2}}\frac{1}{\pi w^2},\\
                &amp;= \frac{1}{\pi w^2} ( 1-e^{-\frac{w^2}{2\sigma^2}} ),\\
                &amp;= \frac{\beta}{\pi w^2},
  \end{align}</div><p>where we define the factor <span class="math notranslate nohighlight">\(\beta = 1-e^{-\frac{w^2}{2\sigma^2}}\)</span>. Further one can compute analytic expressions for the variance and variance of the variance of the harmonic mean estimator for this particular case (see paper for details). This therefore allows one to easily compare evidence and evidence variance estimates to the true evidence and evidence variance – as is done here.</p>
<p>The analytic evidence is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_analytic_evidence</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>

 <span class="n">ln_norm_lik</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">ndim</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>

 <span class="k">return</span> <span class="n">ln_norm_lik</span>
</pre></div>
</div>
<p>where the covariance is initialised by the function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">init_cov</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>

 <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ndim</span><span class="p">,</span><span class="n">ndim</span><span class="p">))</span>
 <span class="n">diag_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
 <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">diag_cov</span><span class="p">)</span>

 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
     <span class="n">cov</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">i</span> <span class="o">*</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">cov</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
     <span class="n">cov</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cov</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

 <span class="k">return</span> <span class="n">cov</span>
</pre></div>
</div>
<p>The log-prior is given by the most straightforward flat uniform prior with infinite extent. We may then combine the log-likelihood and log-prior functions to define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">inv_cov</span><span class="p">):</span>

 <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inv_cov</span><span class="p">,</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="mf">2.0</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">ndim</span> <span class="o">*</span> <span class="n">nchains</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">))</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">inv_cov</span><span class="p">])</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from a uniform area of size representative of the region over which the posterior has large support.</p>
<p>As this is a Gaussian posterior the Hyper-spherical model is an obvious choice. Hence, no cross-validation is necessary and the model can be trained immediately. Having now sucessfully trained the network, we can make a prediction (fit) of the optimal (learnt) container function <span class="math notranslate nohighlight">\(\psi\)</span> – <em>i.e.</em> the optimal hyper-parameter configuration and optimal model class – by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">domains</span><span class="p">)</span>
<span class="n">fit_success</span><span class="p">,</span> <span class="n">objective</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">chains_train</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">chains_train</span><span class="o">.</span><span class="n">ln_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>This container function is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>The <a class="reference external" href="https://www.sfu.ca/~ssurjano/rosen.html">Rosenbrock function</a> is a pathological function often used for benchmarking of algorithms. The functional form is given by</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x) = \sum_{i=1}^{d-1} \bigg [ 100(x_{i+1} - x_{i}^2)^2 + (x_i - 1)^2 \bigg ]\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the dimension of the function and the input domain is usually taken to be <span class="math notranslate nohighlight">\(x_i \in [-5.0, 10.0] \: \; \forall i = 1, \dots, d\)</span>. The Rosenbrock function is difficult in the sense that convergence to the unimodal minimum is difficult.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>by definition the global minimum is trivially given by <span class="math notranslate nohighlight">\(\mathcal{L}(x^{\text{min}}) = 0, \: \text{at} \: x^{\text{min}} = (1,\dots,1)\)</span>.</p>
</div>
<p>The log-likelihood function is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">100.0</span><span class="p">):</span>

     <span class="n">f</span> <span class="o">=</span> <span class="mf">0.0</span>

     <span class="k">for</span> <span class="n">i_dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
             <span class="n">f</span> <span class="o">+=</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i_dim</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i_dim</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>

     <span class="k">return</span> <span class="o">-</span><span class="n">f</span>
</pre></div>
</div>
<p>where the Rosenbrock function <span class="math notranslate nohighlight">\(\mathcal{L}(x)\)</span> is inverted to <span class="math notranslate nohighlight">\(-\mathcal{L}(x)\)</span> so as to form a sensible likelihood function – <em>i.e.</em> a function which converges to a global maximum rather than a global minimum. In the example at hand we assume a simple uniform log-prior defined such that,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior_uniform</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=-</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mf">15.0</span><span class="p">):</span>

     <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">xmin</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">xmax</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">ymin</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">ymax</span><span class="p">:</span>
             <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span> <span class="p">(</span><span class="n">xmax</span> <span class="o">-</span> <span class="n">xmin</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ymax</span> <span class="o">-</span> <span class="n">ymin</span><span class="p">)</span> <span class="p">)</span>
     <span class="k">else</span><span class="p">:</span>
             <span class="k">return</span> <span class="mf">0.0</span>
</pre></div>
</div>
<p>where the log-prior is uniform over <span class="math notranslate nohighlight">\(x_1 \in [-10.0, 10.0] \: \text{and} \: x_2 \in [-5.0, 15.0]\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>One should note that for <span class="math notranslate nohighlight">\(d \gg 1\)</span> uniform priors quickly become very informative, and as such often constitute poor choices – or should at the very least be chosen carefully.</p>
</div>
<p>Additionally we provide an alternate log-prior in the form of a simple Gaussian prior defined such that</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">5.</span><span class="p">):</span>

     <span class="k">return</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">,</span> <span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, combining the log-likelihood and log-prior functions we can define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ln_prior</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">100.0</span><span class="p">):</span>

     <span class="n">ln_L</span> <span class="o">=</span> <span class="n">ln_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>

     <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">ln_L</span><span class="p">):</span>
             <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
     <span class="k">else</span><span class="p">:</span>
             <span class="k">return</span> <span class="n">ln_prior</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">ln_L</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">ndim</span> <span class="o">*</span> <span class="n">nchains</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">ln_prior</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from a uniform area of size representative of the region over which the posterior has large support.</p>
<p>The cross-validation step allows <strong>Harmonic</strong> to compute the optimal hyper-parameter configuration for a certain class of model for a given set of posterior samples.</p>
<p>There are two main stages to this cross-validation process. First the MCMC chains (in this case from emcee) are configured</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chains</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Chains</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">chains</span><span class="o">.</span><span class="n">add_chains_3d</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">lnprob</span><span class="p">)</span>
<span class="n">chains_train</span><span class="p">,</span> <span class="n">chains_test</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">training_proportion</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>before being used as training data to train a network to predict optimal configurations of the hyper-parameters associated with the model class. This is done by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">validation_variances</span> <span class="o">=</span>
                 <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
                                           <span class="n">domain</span><span class="p">,</span>
                                           <span class="n">hyper_parameters</span><span class="p">,</span>
                                           <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                                           <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">KernelDensityEstimate</span><span class="p">,</span>
                                           <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">best_hyper_param_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances</span><span class="p">)</span>
<span class="n">best_hyper_param</span> <span class="o">=</span> <span class="n">hyper_parameters</span><span class="p">[</span><span class="n">best_hyper_param_ind</span><span class="p">]</span>
</pre></div>
</div>
<p>In this case we choose to used the Kernel Density Estimate (KDE) though others could be selected at this stage with ease.</p>
<p>Finally the now sucessfully trained network is used to make a prediction (fit) the optimal (learnt) container function <span class="math notranslate nohighlight">\(\psi\)</span> – <em>i.e.</em> the optimal hyper-parameter configuration – by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">KernelDensityEstimate</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">domain</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param</span><span class="p">)</span>
<span class="n">fit_success</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">chains_train</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">chains_train</span><span class="o">.</span><span class="n">ln_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>This container function is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>The <a class="reference external" href="https://www.sfu.ca/~ssurjano/rastr.html">Rastrigin function</a> is a particularly tricky function often used for benchmarking of algorithms. The functional form is given by</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x) = 10 d + \sum_{i=1}^{d} \bigg [ x_i^2 - 10 \cos ( 2 \pi x_i ) \bigg ]\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the dimension of the function and the input domain is taken in this case to be <span class="math notranslate nohighlight">\(x_i \in [-6.0, 6.0] \: \; \forall i = 1, \dots, d\)</span>. The Rastrigin function is particularly pathological as it is highly multimodal however the maxima locations are regularly distributed which simplifies the problem somewhat.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>by definition the global minimum is trivially given by <span class="math notranslate nohighlight">\(\mathcal{L}(x^{\text{min}}) = 0, \: \text{at} \: x^{\text{min}} = (0,\dots,0)\)</span>.</p>
</div>
<p>The log-likelihood function is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>

     <span class="n">f</span> <span class="o">=</span> <span class="mf">10.0</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span>

     <span class="k">for</span> <span class="n">i_dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
             <span class="n">f</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i_dim</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">10.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i_dim</span><span class="p">]</span> <span class="p">)</span>

     <span class="k">return</span> <span class="o">-</span><span class="n">f</span>
</pre></div>
</div>
<p>where the Rastrigin function <span class="math notranslate nohighlight">\(\mathcal{L}(x)\)</span> is inverted to <span class="math notranslate nohighlight">\(-\mathcal{L}(x)\)</span> so as to form a sensible likelihood function – <em>i.e.</em> a function which converges to a global maximum rather than a global minimum. In the example at hand we assume a simple uniform log-prior defined such that,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior_uniform</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=-</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=-</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mf">6.0</span><span class="p">):</span>

     <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">xmin</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">xmax</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">ymin</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">ymax</span><span class="p">:</span>
             <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span> <span class="p">(</span><span class="n">xmax</span> <span class="o">-</span> <span class="n">xmin</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ymax</span> <span class="o">-</span> <span class="n">ymin</span><span class="p">)</span> <span class="p">)</span>
     <span class="k">else</span><span class="p">:</span>
             <span class="k">return</span> <span class="mf">0.0</span>
</pre></div>
</div>
<p>where the prior is uniform over <span class="math notranslate nohighlight">\(x_i \in [-6.0, 6.0] \: \; \forall i = 1, \dots, d\)</span>. In practice however this prior need not necessarily be uniform.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>One should note that for <span class="math notranslate nohighlight">\(d \gg 1\)</span> uniform priors quickly become very informative, and as such often constitute poor choices.</p>
</div>
<p>Finally, combining the log-likelihood and log-prior functions we can define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ln_prior</span><span class="p">):</span>

     <span class="n">ln_L</span> <span class="o">=</span> <span class="n">ln_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

     <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">ln_L</span><span class="p">):</span>
             <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
     <span class="k">else</span><span class="p">:</span>
             <span class="k">return</span> <span class="n">ln_prior</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">ln_L</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">ndim</span> <span class="o">*</span> <span class="n">nchains</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.5</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posteriargs</span><span class="o">=</span><span class="p">[</span><span class="n">ln_prior</span><span class="p">])</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from a uniform area of size representative of the region over which the posterior has large support.</p>
<p>The cross-validation step allows <strong>Harmonic</strong> to compute the optimal hyper-parameter configuration for a certain class of model for a given set of posterior samples.</p>
<p>There are two main stages to this cross-validation process. First the MCMC chains (in this case from emcee) are configured</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chains</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Chains</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">chains</span><span class="o">.</span><span class="n">add_chains_3d</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">lnprob</span><span class="p">)</span>
<span class="n">chains_train</span><span class="p">,</span> <span class="n">chains_test</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">training_proportion</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>before being used as training data to train a network to predict optimal configurations of the hyper-parameters associated with the model class. This is done by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">validation_variances</span> <span class="o">=</span>
                 <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
                                           <span class="n">domain</span><span class="p">,</span>
                                           <span class="n">hyper_parameters</span><span class="p">,</span>
                                           <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                                           <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">KernelDensityEstimate</span><span class="p">,</span>
                                           <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">best_hyper_param_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances</span><span class="p">)</span>
<span class="n">best_hyper_param</span> <span class="o">=</span> <span class="n">hyper_parameters</span><span class="p">[</span><span class="n">best_hyper_param_ind</span><span class="p">]</span>
</pre></div>
</div>
<p>In this case we choose to used the Kernel Density Estimate (KDE) though others could be selected at this stage with ease.</p>
<p>Finally the now sucessfully trained network is used to make a prediction (fit) the optimal (learnt) container function <span class="math notranslate nohighlight">\(\psi\)</span> – <em>i.e.</em> the optimal hyper-parameter configuration – by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">KernelDensityEstimate</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">domain</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param</span><span class="p">)</span>
<span class="n">fit_success</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">chains_train</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">chains_train</span><span class="o">.</span><span class="n">ln_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>This container function is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-3" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-3" name="0-3" role="tabpanel" tabindex="0"><p>The Normal-Gamma distirbution is an interesting example as it is one for which the original harmonic mean estimator catastrophically failed. Further it is claimed that the harmonic mean evidence estimator is insensitive to the prior and should be avoided – a question addressed within this example. The Normal-Gamma posterior configuration is composed of a likelihood defined as</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{x}|\mu,\tau) = \prod_{i=1}^n p(x_i \vert \mu, \tau)
= \frac{\tau^{n/2}}{(2\pi)^{n/2}} e^{-\frac{\tau}{2}\sum_{i=1}^n(x_i-\mu)^2}
=\frac{\tau^{n/2}}{(2\pi)^{n/2}} e^{-\frac{\tau}{2} n (s^2 + (\bar{x} - \mu)^2)}\]</div>
<p>and a prior defined as</p>
<div class="math notranslate nohighlight">
\[\pi(\mu, \tau) = \frac{{\beta_0}^{\alpha_0}           \sqrt{\tau_0}}{\Gamma(\alpha_0)\sqrt{2\pi}}\tau^{\alpha_0-1/2} e^{-\beta_0\tau}
e^{-\frac{\tau_0\tau(\mu-\mu_0)^2}{2}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_0, \beta_0, \tau_0\)</span> and <span class="math notranslate nohighlight">\(\mu_0\)</span> are parameters that define the prior and <span class="math notranslate nohighlight">\(n\)</span> is the number of data points <span class="math notranslate nohighlight">\(x_i\)</span> we have.
A useful property of this particular posterior is that it permits a closed for analytic expression for the evidence, given as</p>
<div class="math notranslate nohighlight">
\[z = \frac{\Gamma(\alpha_n)}{\Gamma(\alpha_0)}\frac{\beta^\alpha_0}{\beta_n^{\alpha_n}}\left(\frac{\tau_0}{\tau_n}\right)^{1/2}(2\pi)^{-n/2}\]</div>
<p>where for data mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> we have defined,</p>
<div class="math notranslate nohighlight">
\[\tau_n = \tau_0 + n, \quad \alpha_n = \alpha_0 + n/2, \quad b_n = b_0 +  \frac{1}{2}\sum_{i=1}^n(x_i - \bar{x})^2 + \frac{\tau_0 n(\bar{x}-\mu_0)^2}{2(\tau_0 + n)}\]</div>
<p>This therefore allows one to easily compare evidence estimates to the true evidence – as is done here. A DAG for this problem is presented below.</p>
<a class="reference internal image-reference" href="../_images/hbm_normal_gamma.svg"><img alt="../_images/hbm_normal_gamma.svg" class="align-center" src="../_images/hbm_normal_gamma.svg" width="43%" /></a>
<p>The log-likelihood function is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_likelihood</span><span class="p">(</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>

     <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_n</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_std</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_mean</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>The log-prior is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">prior_params</span><span class="p">):</span>

             <span class="k">if</span> <span class="n">tau</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                     <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

             <span class="n">mu_0</span><span class="p">,</span> <span class="n">tau_0</span><span class="p">,</span> <span class="n">alpha_0</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">=</span> <span class="n">prior_params</span>
             <span class="n">ln_pr</span> <span class="o">=</span> <span class="n">alpha_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta_0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau_0</span><span class="p">)</span>
             <span class="n">ln_pr</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">sp</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">alpha_0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
             <span class="n">ln_pr</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alpha_0</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
             <span class="n">ln_pr</span> <span class="o">+=</span> <span class="o">-</span><span class="n">beta_0</span> <span class="o">*</span> <span class="n">tau</span>
             <span class="n">ln_pr</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">tau_0</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

             <span class="k">return</span> <span class="n">ln_pr</span>
</pre></div>
</div>
<p>where the term <em>prior_params</em> is a tuple which stores the parameters <span class="math notranslate nohighlight">\(\alpha_0, \beta_0, \tau_0\)</span> and <span class="math notranslate nohighlight">\(\mu_0\)</span>.</p>
<p>We may then combine the log-likelihood and log-prior functions to define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">prior_params</span><span class="p">):</span>

             <span class="n">mu</span><span class="p">,</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">theta</span>
             <span class="n">ln_pr</span> <span class="o">=</span> <span class="n">ln_prior</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">prior_params</span><span class="p">)</span>

             <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">ln_pr</span><span class="p">):</span>
                     <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

             <span class="n">ln_L</span> <span class="o">=</span> <span class="n">ln_likelihood</span><span class="p">(</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>

             <span class="k">return</span>  <span class="n">ln_L</span> <span class="o">+</span> <span class="n">ln_pr</span>
</pre></div>
</div>
<p>Further as discussed we can explicitly calculate the analytic evidence by defining a function such as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_analytic_evidence</span><span class="p">(</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">prior_params</span><span class="p">):</span>

             <span class="n">mu_0</span><span class="p">,</span> <span class="n">tau_0</span><span class="p">,</span> <span class="n">alpha_0</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">=</span> <span class="n">prior_params</span>
             <span class="n">tau_n</span>  <span class="o">=</span> <span class="n">tau_0</span>  <span class="o">+</span> <span class="n">x_n</span>
             <span class="n">alpha_n</span> <span class="o">=</span> <span class="n">alpha_0</span> <span class="o">+</span> <span class="n">x_n</span><span class="o">/</span><span class="mi">2</span>
             <span class="n">beta_n</span>  <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_n</span> <span class="o">*</span> <span class="n">x_std</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">tau_0</span> <span class="o">*</span> <span class="n">x_n</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_mean</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">tau_0</span> <span class="o">+</span> <span class="n">x_n</span><span class="p">))</span>
             <span class="n">ln_z</span>  <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">alpha_n</span><span class="p">)</span> <span class="o">-</span> <span class="n">sp</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">alpha_0</span><span class="p">)</span>
             <span class="n">ln_z</span> <span class="o">+=</span> <span class="n">alpha_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta_0</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha_n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta_n</span><span class="p">)</span>
             <span class="n">ln_z</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau_0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau_n</span><span class="p">)</span>
             <span class="n">ln_z</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x_n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

             <span class="k">return</span> <span class="n">ln_z</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_mean</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">x_std</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> <span class="n">x_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x_n</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nchains</span><span class="p">)]</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">prior_params</span><span class="p">))</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from a uniform area of size representative of the region over which the posterior has large support.</p>
<p>The cross-validation step allows <strong>Harmonic</strong> to compute the optimal hyper-parameter configuration for a certain class of model for a given set of posterior samples.</p>
<p>There are two main stages to this cross-validation process. First the MCMC chains (in this case from emcee) are configured</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chains</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Chains</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">chains</span><span class="o">.</span><span class="n">add_chains_3d</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">lnprob</span><span class="p">)</span>
<span class="n">chains_train</span><span class="p">,</span> <span class="n">chains_test</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">training_proportion</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
<p>before being used as training data to train a network to predict the optimal model class and optimal configuration of the hyper-parameters associated with the model class. This is done by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#! Make predictions for MGMM model class</span>
<span class="c1">#! -------------------------------------</span>
<span class="n">validation_variances_MGMM</span> <span class="o">=</span>
             <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
                 <span class="n">domains_MGMM</span><span class="p">,</span>
                 <span class="n">hyper_parameters_MGMM</span><span class="p">,</span>
                 <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                 <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModifiedGaussianMixtureModel</span><span class="p">,</span>
                 <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">best_hyper_param_MGMM_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances_MGMM</span><span class="p">)</span>
<span class="n">best_hyper_param_MGMM</span> <span class="o">=</span> <span class="n">hyper_parameters_MGMM</span><span class="p">[</span><span class="n">best_hyper_param_MGMM_ind</span><span class="p">]</span>

<span class="c1">#! Make predictions for Hyper-sphere model class</span>
<span class="c1">#! ---------------------------------------------</span>
<span class="n">validation_variances_sphere</span> <span class="o">=</span>
             <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
                 <span class="n">domains_sphere</span><span class="p">,</span>
                 <span class="n">hyper_parameters_sphere</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                 <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">,</span>
                 <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">best_hyper_param_sphere_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances_sphere</span><span class="p">)</span>
<span class="n">best_hyper_param_sphere</span> <span class="o">=</span> <span class="n">hyper_parameters_sphere</span><span class="p">[</span><span class="n">best_hyper_param_sphere_ind</span><span class="p">]</span>
</pre></div>
</div>
<p>In this case we perform cross-validation for both the MGMM and hyper-sphere model classes, from which one can select the optimal model class and the optimal set of hyper-parameters associated with that class.</p>
<p>Finally the now sucessfully trained network is used to make a prediction (fit) the optimal (learnt) container function <span class="math notranslate nohighlight">\(\psi\)</span> – <em>i.e.</em> the optimal hyper-parameter configuration and optimal model class – by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">best_var_MGMM</span> <span class="o">=</span> <span class="n">validation_variances_MGMM</span><span class="p">[</span><span class="n">best_hyper_param_MGMM_ind</span><span class="p">]</span>
<span class="n">best_var_sphere</span> <span class="o">=</span> <span class="n">validation_variances_sphere</span><span class="p">[</span><span class="n">best_hyper_param_sphere_ind</span><span class="p">]</span>

<span class="c1">#! Select the optimal (minimum variance) model class</span>
<span class="c1">#! -------------------------------------------------</span>
<span class="k">if</span> <span class="n">best_var_MGMM</span> <span class="o">&lt;</span> <span class="n">best_var_sphere</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModifiedGaussianMixtureModel</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">domains_MGMM</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param_MGMM</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">domains_sphere</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param_sphere</span><span class="p">)</span>
<span class="n">fit_success</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">chains_train</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">chains_train</span><span class="o">.</span><span class="n">ln_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>This container function is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-4" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-4" name="0-4" role="tabpanel" tabindex="0"><p>We consider another example where the original harmonic mean estimator was shown to fail catastrophically. In particular, we consider non-nested linear regression models for the <strong>Radiata pine</strong> data, which is another common benchmark data-set, and show that our learnt harmonic mean estimator is highly accurate.</p>
<p>For <span class="math notranslate nohighlight">\(n=42\)</span> trees, the Radiata pine data-set includes measurements of the maximum compression strength parallel to the grain <span class="math notranslate nohighlight">\(y_i\)</span>, density <span class="math notranslate nohighlight">\(x_i\)</span> and resin-adjusted density <span class="math notranslate nohighlight">\(z_i\)</span>, for specimen <span class="math notranslate nohighlight">\(i \in \{1, \ldots, n\}\)</span>.  The question at hand is whether density or resin-adjusted density is a better predictor of compression strength. This motivates two Gaussian linear regression models:</p>
<div class="math notranslate nohighlight">
\begin{align}
&amp;M_1 : y_i = \alpha + \beta(x_i - \bar{x}) + \epsilon_i, \epsilon_i \sim \text{N}(0, \tau^{-1}), \\
&amp;M_2 : y_i = \gamma + \delta(z_i - \bar{z}) + \eta_i, \eta_i \sim \text{N}(0, \lambda^{-1}),
\end{align}</div><p>where <span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\)</span>, <span class="math notranslate nohighlight">\(\bar{z} = \frac{1}{n} \sum_{i=1}^n z_i\)</span>, and <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> denote the precision (inverse variance) of the noise for the respective models.</p>
<p>For Model 1, Gaussian priors are assumed for the bias and linear terms:</p>
<div class="math notranslate nohighlight">
       \begin{align}
       &amp;\alpha \sim \text{N}\bigl(\mu_\alpha, (r_0 \tau)^{-1}\bigr), \\
       &amp;\beta  \sim \text{N}\bigl(\mu_\beta, (s_0 \tau)^{-1}\bigr),
       \end{align}</div><p>with means <span class="math notranslate nohighlight">\(\mu_\alpha = 3000\)</span> and <span class="math notranslate nohighlight">\(\mu_\beta = 185\)</span>, and precision scales <span class="math notranslate nohighlight">\(r_0 = 0.06\)</span> and <span class="math notranslate nohighlight">\(s_0 = 6\)</span>.  A gamma prior <span class="math notranslate nohighlight">\(\tau \sim \text{Ga}(a_0, b_0)\)</span> is assumed for the noise precision with shape <span class="math notranslate nohighlight">\(a_0 = 3\)</span> and rate <span class="math notranslate nohighlight">\(b_0 = 2 \times 300^2\)</span>. The joint prior for <span class="math notranslate nohighlight">\((\alpha, \beta, \tau)\)</span> then reads:</p>
<div class="math notranslate nohighlight">
        \begin{align}
        \pi(\alpha, \beta, \tau) &amp;= \pi(\alpha, \beta | \tau) \pi(\tau) = \pi(\alpha | \tau) \pi(\beta | \tau) \pi(\tau) \\
                         &amp;= \frac{(b_0\tau_0)^{a_0} (r_0 s_0)^{1/2} }{2 \pi \Gamma(a_0)} \exp\bigl(-b_0 \tau\bigr) \exp\biggl(-\frac{\tau}{2}\Bigl(r_0(\alpha-\mu_\alpha)^2 + s_0(\beta-\mu_\beta)^2\Bigr)\biggr).
\end{align}</div><p>The likelihood for Model 1 is given by</p>
<div class="math notranslate nohighlight">
        \begin{align}
        \mathcal{L}({x}, {y}) &amp;= \prod_{i=1}^n \text{P}(x_i, y_i | \alpha, \beta, \tau), \\
                      &amp;= \prod_{i=1}^n \sqrt{\frac{\tau}{2\pi}} \exp\Bigl(- \frac{\tau}{2} \bigl(y_i - \alpha - \beta (x_i - \bar{x})\bigr)^2\Bigr), \\
                      &amp;= \Bigl(\frac{\tau}{2\pi}\Bigr)^{n/2} \exp\biggl(- \frac{\tau}{2} \sum_{i=1}^n \bigl(y_i - \alpha - \beta (x_i - \bar{x})\bigr)^2\biggr),
\end{align}</div><p>where <span class="math notranslate nohighlight">\(x = (x_1, \dots, x_n)^\text{T}\)</span> and <span class="math notranslate nohighlight">\(y = (y_1, \dots, y_n)^\text{T}\)</span>.  For Model 2, the priors adopted for <span class="math notranslate nohighlight">\((\gamma, \delta, \lambda)\)</span> are the same as those adopted for <span class="math notranslate nohighlight">\((\alpha, \beta, \tau)\)</span> of Model 1, respectively, with the same hyper-parameters.  The likelihood for Model 2 again takes an identical form to Model 1, and is presented in the DAG below.</p>
<a class="reference internal image-reference" href="../_images/hbm_radiata_pine.svg"><img alt="../_images/hbm_radiata_pine.svg" class="align-center" src="../_images/hbm_radiata_pine.svg" width="50%" /></a>
<p>The log-likelihood function is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>

 <span class="n">ln_like</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
 <span class="n">ln_like</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
 <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
 <span class="n">ln_like</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">s</span>

 <span class="k">return</span> <span class="n">ln_like</span>
</pre></div>
</div>
<p>The combined log-prior is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">,</span> <span class="n">a_0</span><span class="p">,</span> <span class="n">b_0</span><span class="p">):</span>

 <span class="k">if</span> <span class="n">tau</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

 <span class="n">ln_pr</span> <span class="o">=</span> <span class="n">a_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">b_0</span><span class="p">)</span>
 <span class="n">ln_pr</span> <span class="o">+=</span> <span class="n">a_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
 <span class="n">ln_pr</span> <span class="o">-=</span> <span class="n">b_0</span> <span class="o">*</span> <span class="n">tau</span>
 <span class="n">ln_pr</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
 <span class="n">ln_pr</span> <span class="o">-=</span> <span class="n">sp</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">a_0</span><span class="p">)</span>
 <span class="n">ln_pr</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">r_0</span><span class="p">)</span>
 <span class="n">ln_pr</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s_0</span><span class="p">)</span>
 <span class="n">ln_pr</span> <span class="o">-=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="p">(</span><span class="n">r_0</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">s_0</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

 <span class="k">return</span> <span class="n">ln_pr</span>
</pre></div>
</div>
<p>We may then combine the log-likelihood and log-prior functions to define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">,</span> <span class="n">a_0</span><span class="p">,</span> <span class="n">b_0</span><span class="p">):</span>

 <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">theta</span>
 <span class="n">ln_pr</span> <span class="o">=</span> <span class="n">ln_prior</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">,</span> <span class="n">a_0</span><span class="p">,</span> <span class="n">b_0</span><span class="p">)</span>

 <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">ln_pr</span><span class="p">):</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

 <span class="n">ln_L</span> <span class="o">=</span> <span class="n">ln_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>

 <span class="k">return</span>  <span class="n">ln_L</span> <span class="o">+</span> <span class="n">ln_pr</span>
</pre></div>
</div>
<p>Further as discussed we can explicitly calculate the analytic evidence by defining a function such as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_evidence_analytic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">,</span> <span class="n">a_0</span><span class="p">,</span> <span class="n">b_0</span><span class="p">):</span>

 <span class="n">Q_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">])</span>
 <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">x</span><span class="p">]</span>
 <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">Q_0</span>
 <span class="n">nu_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">Q_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mu_0</span><span class="p">))</span>

 <span class="n">quad_terms</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">mu_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q_0</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mu_0</span><span class="p">)</span> <span class="o">-</span> <span class="n">nu_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">nu_0</span><span class="p">)</span>

 <span class="n">ln_evidence</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
 <span class="n">ln_evidence</span> <span class="o">+=</span> <span class="n">a_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">b_0</span><span class="p">)</span>
 <span class="n">ln_evidence</span> <span class="o">+=</span> <span class="n">sp</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">n</span> <span class="o">+</span> <span class="n">a_0</span><span class="p">)</span> <span class="o">-</span> <span class="n">sp</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">a_0</span><span class="p">)</span>
 <span class="n">ln_evidence</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Q_0</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">M</span><span class="p">))</span>
 <span class="n">ln_evidence</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">a_0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">quad_terms</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">b_0</span><span class="p">)</span>

 <span class="k">return</span> <span class="n">ln_evidence</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos_alpha</span> <span class="o">=</span> <span class="n">mu_0</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tau_prior_mean</span> <span class="o">*</span> <span class="n">r_0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span>
<span class="n">pos_beta</span> <span class="o">=</span> <span class="n">mu_0</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tau_prior_mean</span> <span class="o">*</span> <span class="n">s_0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span>
<span class="n">pos_tau</span> <span class="o">=</span> <span class="n">tau_prior_mean</span> <span class="o">+</span> <span class="n">tau_prior_std</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">pos_alpha</span><span class="p">,</span> <span class="n">pos_beta</span><span class="p">,</span> <span class="n">pos_tau</span><span class="p">]</span>

<span class="k">if</span> <span class="n">model_1</span><span class="p">:</span>
    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">,</span> <span class="n">a_0</span><span class="p">,</span> <span class="n">b_0</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">s_0</span><span class="p">,</span> <span class="n">a_0</span><span class="p">,</span> <span class="n">b_0</span><span class="p">)</span>

<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from the support of each covariate prior.</p>
<p>We adopt the hyper-spherical model, and fit the model hyper-parameters through cross-validation as in other examples. This learnt model is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-5" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-5" name="0-5" role="tabpanel" tabindex="0"><p>We consider the comparison of two logistic regression models using the <strong>Pima Indians</strong> data, which is another common benchmark problem for comparing estimators of the marginal likelihood.  The original harmonic mean estimator has been shown to fail catastrophically for this example, whereas we show here that our learnt harmonic mean estimator is highly accurate.</p>
<p>The Pima Indians data, originally from the National Institute of Diabetes and Digestive and Kidney Diseases, were compiled from a study of indicators of diabetes in <span class="math notranslate nohighlight">\(n=532\)</span> Pima Indian women aged 21 or over.  Seven primary predictors of diabetes were recorded, including: number of prior pregnancies (NP);  plasma glucose concentration (PGC); diastolic blood pressure (BP); triceps skin fold thickness (TST); body mass index (BMI); diabetes pedigree function (DP); and age (AGE).</p>
<p>The probability of diabetes <span class="math notranslate nohighlight">\(p_i\)</span> for person <span class="math notranslate nohighlight">\(i \in \{1, \ldots, n\}\)</span> can be modelled by the standard logistic function</p>
<div class="math notranslate nohighlight">
\[p_i = \frac{1}{1+\exp\bigl(- \theta^\text{T} x_i\bigr)},\]</div>
<p>with covariates <span class="math notranslate nohighlight">\(x_i = (1,x_{i,1}, \dots x_{i,d})^\text{T}\)</span> and parameters <span class="math notranslate nohighlight">\(\theta = (\theta_0, \dots, \theta_d)^\text{T}\)</span>, where <span class="math notranslate nohighlight">\(d\)</span> is the total number of covariates considered.  The likelihood function then reads</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}({y} | {\theta}) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i},\]</div>
<p>where <span class="math notranslate nohighlight">\(y = (y_1, \dots, y_n)^\text{T}\)</span> is the diabetes incidence, (<em>i.e.</em> <span class="math notranslate nohighlight">\(y_i\)</span> is unity if patient <span class="math notranslate nohighlight">\(i\)</span> had diabetes and zero otherwise). An independent multivariate Gaussian prior is assumed for the parameters <span class="math notranslate nohighlight">\(\theta\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[\pi(\theta) = \Bigl(  \frac{\tau}{2\pi} \Bigr)^{d/2} \exp \bigl( - \frac{\tau}{2} \theta^\text{T} \theta \bigr),\]</div>
<p>with precision <span class="math notranslate nohighlight">\(\tau\)</span>. Two different logistic regression models are considered, with different subsets of covariates:</p>
<div class="math notranslate nohighlight">
  \begin{align}
  &amp;\text{Model 1: covariates = \{NP, PGC, BMI, DP\} (and bias),} \\
  &amp;\text{Model 2: covariates = \{NP, PGC, BMI, DP, AGE\} (and bias)}.
  \end{align}</div><p>A graphical representation of Model 2 is illustrated below (Model 1 is similar but does not include the AGE covariate).</p>
<a class="reference internal image-reference" href="../_images/hbm_pima_indian.svg"><img alt="../_images/hbm_pima_indian.svg" class="align-center" src="../_images/hbm_pima_indian.svg" width="50%" /></a>
<p>The log-likelihood function is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

 <span class="n">ln_p</span> <span class="o">=</span> <span class="n">compute_ln_p</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
 <span class="n">ln_pp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ln_p</span><span class="p">))</span>

 <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ln_p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ln_pp</span><span class="p">)</span>
</pre></div>
</div>
<p>The log-prior is given by a multivariate Gaussian, <em>e.g.</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>

 <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau</span><span class="o">/</span><span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">theta</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
<p>We may then combine the log-likelihood and log-prior functions to define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

 <span class="n">ln_pr</span> <span class="o">=</span> <span class="n">ln_prior</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
 <span class="n">ln_L</span> <span class="o">=</span> <span class="n">ln_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

 <span class="k">return</span> <span class="n">ln_pr</span> <span class="o">+</span> <span class="n">ln_L</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">model_1</span><span class="p">:</span>
     <span class="n">pos_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">pos_0</span><span class="p">,</span> <span class="n">pos_1</span><span class="p">,</span> <span class="n">pos_2</span><span class="p">,</span> <span class="n">pos_3</span><span class="p">,</span> <span class="n">pos_4</span><span class="p">]</span>

<span class="k">else</span><span class="p">:</span>
     <span class="n">pos_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">pos_0</span><span class="p">,</span> <span class="n">pos_1</span><span class="p">,</span> <span class="n">pos_2</span><span class="p">,</span> <span class="n">pos_3</span><span class="p">,</span> <span class="n">pos_4</span><span class="p">,</span> <span class="n">pos_5</span><span class="p">]</span>

<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from the support of each covariate prior.</p>
<p>The cross-validation step allows <strong>Harmonic</strong> to compute the optimal hyper-parameter configuration for a certain class of model for a given set of posterior samples. There are two main stages to this cross-validation process. First the MCMC chains (in this case from emcee) are configured</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chains</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Chains</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">chains</span><span class="o">.</span><span class="n">add_chains_3d</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">lnprob</span><span class="p">)</span>
<span class="n">chains_train</span><span class="p">,</span> <span class="n">chains_test</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">training_proportion</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>before being used as training data to train a network to predict optimal configurations of the hyper-parameters associated with the model class. This is done by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># MGMM model</span>
 <span class="n">validation_variances_MGMM</span> <span class="o">=</span>
     <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
         <span class="n">domains_MGMM</span><span class="p">,</span>
         <span class="n">hyper_parameters_MGMM</span><span class="p">,</span>
         <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
         <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModifiedGaussianMixtureModel</span><span class="p">,</span>
         <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 <span class="n">best_hyper_param_MGMM_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances_MGMM</span><span class="p">)</span>
 <span class="n">best_hyper_param_MGMM</span> <span class="o">=</span> <span class="n">hyper_parameters_MGMM</span><span class="p">[</span><span class="n">best_hyper_param_MGMM_ind</span><span class="p">]</span>

 <span class="c1"># Hyper-spherical model</span>
 <span class="n">validation_variances_sphere</span> <span class="o">=</span>
     <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
         <span class="n">domains_sphere</span><span class="p">,</span>
         <span class="n">hyper_parameters_sphere</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
         <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">,</span>
         <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 <span class="n">best_hyper_param_sphere_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances_sphere</span><span class="p">)</span>
 <span class="n">best_hyper_param_sphere</span> <span class="o">=</span> <span class="n">hyper_parameters_sphere</span><span class="p">[</span><span class="n">best_hyper_param_sphere_ind</span><span class="p">]</span>
</pre></div>
</div>
<p>In this case we adopt cross-validation to select between the MGMM and Hyper-spherical models, as it is not necessarily clear which is more effective. The most effective model is selected by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">best_var_MGMM</span> <span class="o">=</span> <span class="n">validation_variances_MGMM</span><span class="p">[</span><span class="n">best_hyper_param_MGMM_ind</span><span class="p">]</span>
<span class="n">best_var_sphere</span> <span class="o">=</span> <span class="n">validation_variances_sphere</span><span class="p">[</span><span class="n">best_hyper_param_sphere_ind</span><span class="p">]</span>
<span class="k">if</span> <span class="n">best_var_MGMM</span> <span class="o">&lt;</span> <span class="n">best_var_sphere</span><span class="p">:</span>
     <span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModifiedGaussianMixtureModel</span><span class="p">(</span>
     <span class="n">ndim</span><span class="p">,</span> <span class="n">domains_MGMM</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param_MGMM</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
     <span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">(</span>
     <span class="n">ndim</span><span class="p">,</span> <span class="n">domains_sphere</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param_sphere</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally the now sucessfully trained network is used to make a prediction (fit) the optimal (learnt) container function <span class="math notranslate nohighlight">\(\psi\)</span> – <em>i.e.</em> the optimal hyper-parameter configuration – by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fit_success</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">chains_train</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">chains_train</span><span class="o">.</span><span class="n">ln_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>This learnt container function is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div></div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tutorials/cross-validation_learnt_model.html" class="btn btn-neutral float-left" title="Model Cross-validation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../api/index.html" class="btn btn-neutral float-right" title="Namespaces" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Jason D. McEwen, Christopher G. R. Wallis, Matthew A. Price, Matthew M. Docherty.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>