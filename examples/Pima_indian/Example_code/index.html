<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cross-Validation &mdash; Harmonic 1.0.2 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom_tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/dark_mode_css/general.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/dark_mode_css/dark.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script src="../../../_static/dark_mode_js/default_light.js"></script>
        <script src="../../../_static/dark_mode_js/theme_switcher.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html">
            <img src="../../../_static/harm_badge_simple.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.0.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../background/Harmonic_Estimator/index.html">Harmonic Mean Estimator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../background/Machine_Learning/index.html">Learnt Container Function</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Jupyter Notebooks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Benchmark Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">Namespaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Changelog</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/changes.html">GitHub History</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Harmonic</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Cross-Validation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/examples/Pima_indian/Example_code/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p>We consider the comparison of two logistic regression models using the <strong>Pima Indians</strong> data, which is another common benchmark problem for comparing estimators of the marginal likelihood.  The original harmonic mean estimator has been shown to fail catastrophically for this example, whereas we show here that our learnt harmonic mean estimator is highly accurate.</p>
<p>The Pima Indians data, originally from the National Institute of Diabetes and Digestive and Kidney Diseases, were compiled from a study of indicators of diabetes in <span class="math notranslate nohighlight">\(n=532\)</span> Pima Indian women aged 21 or over.  Seven primary predictors of diabetes were recorded, including: number of prior pregnancies (NP);  plasma glucose concentration (PGC); diastolic blood pressure (BP); triceps skin fold thickness (TST); body mass index (BMI); diabetes pedigree function (DP); and age (AGE).</p>
<p>The probability of diabetes <span class="math notranslate nohighlight">\(p_i\)</span> for person <span class="math notranslate nohighlight">\(i \in \{1, \ldots, n\}\)</span> can be modelled by the standard logistic function</p>
<div class="math notranslate nohighlight">
\[p_i = \frac{1}{1+\exp\bigl(- \theta^\text{T} x_i\bigr)},\]</div>
<p>with covariates <span class="math notranslate nohighlight">\(x_i = (1,x_{i,1}, \dots x_{i,d})^\text{T}\)</span> and parameters <span class="math notranslate nohighlight">\(\theta = (\theta_0, \dots, \theta_d)^\text{T}\)</span>, where <span class="math notranslate nohighlight">\(d\)</span> is the total number of covariates considered.  The likelihood function then reads</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}({y} | {\theta}) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i},\]</div>
<p>where <span class="math notranslate nohighlight">\(y = (y_1, \dots, y_n)^\text{T}\)</span> is the diabetes incidence, (<em>i.e.</em> <span class="math notranslate nohighlight">\(y_i\)</span> is unity if patient <span class="math notranslate nohighlight">\(i\)</span> had diabetes and zero otherwise). An independent multivariate Gaussian prior is assumed for the parameters <span class="math notranslate nohighlight">\(\theta\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[\pi(\theta) = \Bigl(  \frac{\tau}{2\pi} \Bigr)^{d/2} \exp \bigl( - \frac{\tau}{2} \theta^\text{T} \theta \bigr),\]</div>
<p>with precision <span class="math notranslate nohighlight">\(\tau\)</span>. Two different logistic regression models are considered, with different subsets of covariates:</p>
<div class="math notranslate nohighlight">
  \begin{align}
  &amp;\text{Model 1: covariates = \{NP, PGC, BMI, DP\} (and bias),} \\
  &amp;\text{Model 2: covariates = \{NP, PGC, BMI, DP, AGE\} (and bias)}.
  \end{align}</div><p>A graphical representation of Model 2 is illustrated below (Model 1 is similar but does not include the AGE covariate).</p>
<a class="reference internal image-reference" href="../../../_images/hbm_pima_indian.svg"><img alt="../../../_images/hbm_pima_indian.svg" class="align-center" src="../../../_images/hbm_pima_indian.svg" width="50%" /></a>
<p>The log-likelihood function is given by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

 <span class="n">ln_p</span> <span class="o">=</span> <span class="n">compute_ln_p</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
 <span class="n">ln_pp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ln_p</span><span class="p">))</span>

 <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ln_p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ln_pp</span><span class="p">)</span>
</pre></div>
</div>
<p>The log-prior is given by a multivariate Gaussian, <em>e.g.</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_prior</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>

 <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau</span><span class="o">/</span><span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">theta</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
<p>We may then combine the log-likelihood and log-prior functions to define the log-posterior function simply by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ln_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

 <span class="n">ln_pr</span> <span class="o">=</span> <span class="n">ln_prior</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
 <span class="n">ln_L</span> <span class="o">=</span> <span class="n">ln_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

 <span class="k">return</span> <span class="n">ln_pr</span> <span class="o">+</span> <span class="n">ln_L</span>
</pre></div>
</div>
<p>The first step of our evidence computation requires recovering a relatively small number of samples from the given posterior. This can be done in whatever way the user wishes, the only requirement being that a set of chains each with associated samples is provided for subsequent steps.
In our examples we choose to use the excellent <a class="reference external" href="http://dfm.io/emcee/current/">emcee</a> python package. Utilizing emcee this example recovers samples via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">model_1</span><span class="p">:</span>
     <span class="n">pos_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">pos_0</span><span class="p">,</span> <span class="n">pos_1</span><span class="p">,</span> <span class="n">pos_2</span><span class="p">,</span> <span class="n">pos_3</span><span class="p">,</span> <span class="n">pos_4</span><span class="p">]</span>

<span class="k">else</span><span class="p">:</span>
     <span class="n">pos_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos_5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nchains</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
     <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">pos_0</span><span class="p">,</span> <span class="n">pos_1</span><span class="p">,</span> <span class="n">pos_2</span><span class="p">,</span> <span class="n">pos_3</span><span class="p">,</span> <span class="n">pos_4</span><span class="p">,</span> <span class="n">pos_5</span><span class="p">]</span>

<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nchains</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">ln_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">rstate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">samples_per_chain</span><span class="p">,</span> <span class="n">rstate0</span><span class="o">=</span><span class="n">rstate</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:,:])</span>
<span class="n">lnprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">sampler</span><span class="o">.</span><span class="n">lnprobability</span><span class="p">[:,</span><span class="n">nburn</span><span class="p">:])</span>
</pre></div>
</div>
<p>where the initial positions are drawn randomly from the support of each covariate prior.</p>
<div class="section" id="cross-validation">
<h1>Cross-Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline"></a></h1>
<p>The cross-validation step allows <strong>Harmonic</strong> to compute the optimal hyper-parameter configuration for a certain class of model for a given set of posterior samples. There are two main stages to this cross-validation process. First the MCMC chains (in this case from emcee) are configured</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chains</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Chains</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
<span class="n">chains</span><span class="o">.</span><span class="n">add_chains_3d</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">lnprob</span><span class="p">)</span>
<span class="n">chains_train</span><span class="p">,</span> <span class="n">chains_test</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">training_proportion</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>before being used as training data to train a network to predict optimal configurations of the hyper-parameters associated with the model class. This is done by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># MGMM model</span>
 <span class="n">validation_variances_MGMM</span> <span class="o">=</span>
     <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
         <span class="n">domains_MGMM</span><span class="p">,</span>
         <span class="n">hyper_parameters_MGMM</span><span class="p">,</span>
         <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
         <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModifiedGaussianMixtureModel</span><span class="p">,</span>
         <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 <span class="n">best_hyper_param_MGMM_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances_MGMM</span><span class="p">)</span>
 <span class="n">best_hyper_param_MGMM</span> <span class="o">=</span> <span class="n">hyper_parameters_MGMM</span><span class="p">[</span><span class="n">best_hyper_param_MGMM_ind</span><span class="p">]</span>

 <span class="c1"># Hyper-spherical model</span>
 <span class="n">validation_variances_sphere</span> <span class="o">=</span>
     <span class="n">hm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">chains_train</span><span class="p">,</span>
         <span class="n">domains_sphere</span><span class="p">,</span>
         <span class="n">hyper_parameters_sphere</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
         <span class="n">modelClass</span><span class="o">=</span><span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">,</span>
         <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 <span class="n">best_hyper_param_sphere_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">validation_variances_sphere</span><span class="p">)</span>
 <span class="n">best_hyper_param_sphere</span> <span class="o">=</span> <span class="n">hyper_parameters_sphere</span><span class="p">[</span><span class="n">best_hyper_param_sphere_ind</span><span class="p">]</span>
</pre></div>
</div>
<p>In this case we adopt cross-validation to select between the MGMM and Hyper-spherical models, as it is not necessarily clear which is more effective. The most effective model is selected by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">best_var_MGMM</span> <span class="o">=</span> <span class="n">validation_variances_MGMM</span><span class="p">[</span><span class="n">best_hyper_param_MGMM_ind</span><span class="p">]</span>
<span class="n">best_var_sphere</span> <span class="o">=</span> <span class="n">validation_variances_sphere</span><span class="p">[</span><span class="n">best_hyper_param_sphere_ind</span><span class="p">]</span>
<span class="k">if</span> <span class="n">best_var_MGMM</span> <span class="o">&lt;</span> <span class="n">best_var_sphere</span><span class="p">:</span>
     <span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModifiedGaussianMixtureModel</span><span class="p">(</span>
     <span class="n">ndim</span><span class="p">,</span> <span class="n">domains_MGMM</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param_MGMM</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
     <span class="n">model</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">HyperSphere</span><span class="p">(</span>
     <span class="n">ndim</span><span class="p">,</span> <span class="n">domains_sphere</span><span class="p">,</span> <span class="n">hyper_parameters</span><span class="o">=</span><span class="n">best_hyper_param_sphere</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally the now sucessfully trained network is used to make a prediction (fit) the optimal (learnt) container function <span class="math notranslate nohighlight">\(\psi\)</span> – <em>i.e.</em> the optimal hyper-parameter configuration – by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fit_success</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">chains_train</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">chains_train</span><span class="o">.</span><span class="n">ln_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>This learnt container function is then used with the harmonic mean estimator to construct a robust computation of the Bayesian evidence by</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ev</span> <span class="o">=</span> <span class="n">hm</span><span class="o">.</span><span class="n">Evidence</span><span class="p">(</span><span class="n">chains_test</span><span class="o">.</span><span class="n">nchains</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">ev</span><span class="o">.</span><span class="n">add_chains</span><span class="p">(</span><span class="n">chains_test</span><span class="p">)</span>
<span class="n">ln_evidence</span><span class="p">,</span> <span class="n">ln_evidence_std</span> <span class="o">=</span> <span class="n">ev</span><span class="o">.</span><span class="n">compute_ln_evidence</span><span class="p">()</span>
</pre></div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Jason D. McEwen, Christopher G. R. Wallis, Matthew A. Price, Matthew M. Docherty.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>